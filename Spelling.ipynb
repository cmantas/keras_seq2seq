{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# ignore some Keras warnings regarding deprecations and model saving \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import pickle\n",
    "\n",
    "from helpers import *\n",
    "import importlib\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences from the [tatoeba dataset](https://tatoeba.org/eng/downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 20  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples =200000 # Number of samples to train on.\n",
    "# Path to the datatxt file on disk.\n",
    "data_path = 'sentences.txt'\n",
    "\n",
    "epochs = 100\n",
    "noise = 0.05\n",
    "misspellings_count = 3\n",
    "chunk_size = 40000\n",
    "\n",
    "optimizer='rmsprop'\n",
    "loss_fn='categorical_crossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-pick maximum sequence lengths\n",
    "max_encoder_seq_length = 25\n",
    "max_decoder_seq_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed(data_path):\n",
    "    with open(data_path) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = text_preprocess(lines)\n",
    "    # allow only for a limited count of \n",
    "    allowed_chars = {'.', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', \n",
    "             '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', \n",
    "             '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', \n",
    "             '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    "             'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "             's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}' }\n",
    "    selected = []\n",
    "    for l in lines:\n",
    "        if all([c in allowed_chars for c in l.strip()]) and \\\n",
    "           len(l) < max_encoder_seq_length:\n",
    "            selected.append(l)\n",
    "    # suffle deterministically\n",
    "    Random(0).shuffle(selected)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phrases in dataset:  210139\n",
      "Training phrases:  200000\n",
      "Test phrases:  10139\n",
      "Examples:\n",
      " * leave him alone, please\n",
      " * what do you call love?\n",
      " * tom got in the taxi\n",
      " * we can trust tom\n",
      " * i'd like you to meet tom\n",
      " * the money is terrible\n",
      " * did you go to see him?\n",
      " * monday will be a hot day\n",
      " * tom took mary to dinner\n",
      " * turn on the light please\n"
     ]
    }
   ],
   "source": [
    "all_phrases = load_preprocessed(data_path)\n",
    "input_phrases = all_phrases[:num_samples]\n",
    "test_phrases = all_phrases[num_samples:]\n",
    "print('All phrases in dataset: ', len(all_phrases))\n",
    "print('Training phrases: ', len(input_phrases))\n",
    "print('Test phrases: ', len(test_phrases))\n",
    "\n",
    "print(\"\\n * \".join(['Examples:'] + all_phrases[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique input tokens: 55\n",
      "Number of unique output tokens: 57\n"
     ]
    }
   ],
   "source": [
    "# create doken indices out of all phrases\n",
    "input_token_index = token_index(all_phrases)\n",
    "num_encoder_tokens = len(input_token_index)\n",
    "target_token_index = {'\\t': num_encoder_tokens, \n",
    "                      '\\n': num_encoder_tokens+1, \n",
    "                      **input_token_index}\n",
    "num_decoder_tokens = len(target_token_index)\n",
    "\n",
    "# Keep the count of all the possible input characters\n",
    "\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(num_encoder_tokens, num_decoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder')\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, \n",
    "                        name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', \n",
    "                          name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Next: inference mode (sampling).\n",
    "    # Here's the drill:\n",
    "    # 1) encode input and retrieve initial decoder state\n",
    "    # 2) run one step of decoder with this initial state\n",
    "    # and a \"start of sequence\" token as target.\n",
    "    # Output will be the next target token\n",
    "    # 3) Repeat with the current target token and current states\n",
    "\n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = models(num_encoder_tokens, num_decoder_tokens, latent_dim)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train few epochs on an identity fn with a chunk of the dataset for sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_phrases = input_phrases[:10000]\n",
    "X, Y = vectorize_dataset(train_input_phrases, wrap_with_delims(train_input_phrases),\n",
    "                  input_token_index,target_token_index,\n",
    "                  max_encoder_seq_length, max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 2.0213 - val_loss: 1.8918\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.8122 - val_loss: 1.7021\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 1.6367 - val_loss: 1.5576\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.5195 - val_loss: 1.5098\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.4479 - val_loss: 1.4396\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.3899 - val_loss: 1.3576\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.3278 - val_loss: 1.3101\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.2753 - val_loss: 1.2714\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.2376 - val_loss: 1.2211\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.2204 - val_loss: 1.2292\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.1903 - val_loss: 1.1941\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.1671 - val_loss: 1.1631\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.1278 - val_loss: 1.1229\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.0972 - val_loss: 1.1041\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.0682 - val_loss: 1.0646\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.0414 - val_loss: 1.0933\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 1.0169 - val_loss: 1.0248\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.9911 - val_loss: 0.9943\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.9676 - val_loss: 1.0322\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.9465 - val_loss: 0.9851\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.9260 - val_loss: 0.9447\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.9039 - val_loss: 0.9333\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.8869 - val_loss: 0.9271\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.8688 - val_loss: 0.9321\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.8528 - val_loss: 0.8972\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.8361 - val_loss: 0.9210\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.8211 - val_loss: 0.8923\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.8065 - val_loss: 0.8729\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7908 - val_loss: 0.8623\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7778 - val_loss: 0.8472\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7631 - val_loss: 0.8409\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7500 - val_loss: 0.8421\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7353 - val_loss: 0.8661\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7253 - val_loss: 0.8127\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.7109 - val_loss: 0.8643\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6994 - val_loss: 0.8099\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6859 - val_loss: 0.8128\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6728 - val_loss: 0.8163\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6628 - val_loss: 0.7992\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6500 - val_loss: 0.7912\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6383 - val_loss: 0.7889\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6282 - val_loss: 0.7881\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6155 - val_loss: 0.8044\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.6040 - val_loss: 0.7859\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5931 - val_loss: 0.7873\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5823 - val_loss: 0.7847\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5709 - val_loss: 0.7666\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5590 - val_loss: 0.7654\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5484 - val_loss: 0.7560\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5380 - val_loss: 0.7502\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5258 - val_loss: 0.7717\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5155 - val_loss: 0.7884\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.5057 - val_loss: 0.7655\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4947 - val_loss: 0.7555\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4838 - val_loss: 0.7755\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4741 - val_loss: 0.7451\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4614 - val_loss: 0.7799\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4541 - val_loss: 0.7568\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4430 - val_loss: 0.7619\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4330 - val_loss: 0.7734\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4238 - val_loss: 0.7478\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4134 - val_loss: 0.7329\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.4032 - val_loss: 0.7641\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3954 - val_loss: 0.7691\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3861 - val_loss: 0.7506\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3774 - val_loss: 0.7695\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3679 - val_loss: 0.7377\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3594 - val_loss: 0.7533\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3494 - val_loss: 0.7557\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3423 - val_loss: 0.7440\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3333 - val_loss: 0.7532\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3239 - val_loss: 0.7609\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3172 - val_loss: 0.7562\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3079 - val_loss: 0.7605\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3004 - val_loss: 0.7453\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.2925 - val_loss: 0.7650\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.2854 - val_loss: 0.7455\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2775 - val_loss: 0.7592\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2695 - val_loss: 0.7784\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2623 - val_loss: 0.7700\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2537 - val_loss: 0.7770\n",
      "Epoch 82/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2477 - val_loss: 0.7615\n",
      "Epoch 83/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2402 - val_loss: 0.7859\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2360 - val_loss: 0.7721\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2269 - val_loss: 0.8024\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2214 - val_loss: 0.7805\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2154 - val_loss: 0.7740\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2094 - val_loss: 0.7899\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2006 - val_loss: 0.8054\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1994 - val_loss: 0.7934\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1914 - val_loss: 0.7923\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1847 - val_loss: 0.8013\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1799 - val_loss: 0.8063\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1751 - val_loss: 0.7925\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1682 - val_loss: 0.8021\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1652 - val_loss: 0.8348\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1614 - val_loss: 0.8278\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1536 - val_loss: 0.8278\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1501 - val_loss: 0.8396\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1457 - val_loss: 0.8287\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe4322e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X,Y,\n",
    "                   batch_size=batch_size, epochs=100, \n",
    "                   validation_split=0.1)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"corrector\" fn that will \"translate\" our misspelled input to a right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector = translate_fn(encoder_model, decoder_model, \n",
    "                         input_token_index, target_token_index,\n",
    "                        max_encoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leave help mary for me!',\n",
       " 'what do you call love?',\n",
       " 'tom got in the bair',\n",
       " \"we can't stopting\",\n",
       " \"i'd like you to do this\",\n",
       " 'the money is need you',\n",
       " 'did you thing to you?',\n",
       " 'monday is wearing childed',\n",
       " 'tom told me be on my wir',\n",
       " 'turn on the lights leath']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what our model has learned so far\n",
    "# by trying to \"correct\" some correct phrases\n",
    "[corrector(p) for p in train_input_phrases[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_correct(texts, corrector):\n",
    "    errors = 0.0\n",
    "    for t in texts:\n",
    "        if t != corrector(t): errors += 1\n",
    "    return errors / len(texts)\n",
    "\n",
    "def evaluate_misspelled(texts, corrector):\n",
    "    errors = 0.0\n",
    "    for t in texts:\n",
    "        errored = add_noise_to_string(t, 0.05)\n",
    "        if t != corrector(errored): errors += 1\n",
    "    return errors / len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_correct(train_input_phrases[:100], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_misspelled(train_input_phrases[:100], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_vectorizer_fn(input_token_index, max_encoder_seq_length,\n",
    "                           target_token_index, max_decoder_seq_length):\n",
    "    # create a closure fn that \"knows\" the token indices and seq lengths\n",
    "    def training_vectorizer(input_texts, target_texts):\n",
    "        encoder_input_data = vectorize_batch(input_texts, input_token_index,\n",
    "                                             max_encoder_seq_length)\n",
    "        decoder_input_data = vectorize_batch(target_texts, target_token_index,\n",
    "                                             max_decoder_seq_length)\n",
    "                # same as decoder input data, but offset by one\n",
    "        decoder_output_data = vectorize_batch(target_texts, target_token_index,\n",
    "                                              max_decoder_seq_length, True)\n",
    "        X = [encoder_input_data, decoder_input_data]\n",
    "        Y = decoder_output_data\n",
    "        return X, Y\n",
    "\n",
    "    return training_vectorizer\n",
    "\n",
    "# Create a training_vectorizer that only accepts input and target texts\n",
    "training_vectorizer = training_vectorizer_fn(input_token_index, max_encoder_seq_length,\n",
    "                                             target_token_index, max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_gen(phrases, batch_size, misspellings_count, noise):\n",
    "    \"\"\"Goes through the given phrases, in `batch_size` batches, generating \n",
    "    `misspellings_count` misspelling allongside them.\n",
    "    On each iteration it yields `batch_size`* (1+ misspellings_count) strings: \n",
    "    the original strings and the misspellings generated out of them\"\"\"\n",
    "    for i in range(0, len(phrases), batch_size):\n",
    "        frrom = i\n",
    "        to = i+batch_size\n",
    "        print(\"Yielding phrases from #%d to #%d\" % (frrom, to))\n",
    "        yield create_misspellings(phrases[frrom:to], noise, misspellings_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding phrases from #0 to #3\n",
      "leave him alone, please -> leave him alone, please\n",
      "what do you call love? -> what do you call love?\n",
      "tom got in the taxi -> tom got in the taxi\n",
      "leae him alone, pelase -> leave him alone, please\n",
      "what edo you call love? -> what do you call love?\n",
      "tom got int h taxi -> tom got in the taxi\n",
      "leave him alonec please -> leave him alone, please\n",
      "wat do yuo call love? -> what do you call love?\n",
      "tom got in the taxi -> tom got in the taxi\n"
     ]
    }
   ],
   "source": [
    "tst = batched_gen(input_phrases, 3, 2, 0.07)\n",
    "inp, trgt = next(tst)\n",
    "for i,t in zip(inp, trgt):\n",
    "    print(i, '->',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_gen(phrases, batch_size, misspellings_count, noise,\n",
    "                   training_vectorizer):\n",
    "    \"\"\"Creates vextorized batches of phrases (that are wrapped with delims)\"\"\"\n",
    "    # Create a generator of misspelled strings from the input phrases\n",
    "    gen = batched_gen(phrases, batch_size, misspellings_count, noise)\n",
    "    \n",
    "    # Go through all the input phrases, generatiing misspellings, vectorizing them\n",
    "    # and yielding each batch\n",
    "    for input_phrases, target_phrases in gen:\n",
    "        target_phrases = wrap_with_delims(target_phrases)\n",
    "        X, Y = training_vectorizer(input_phrases, target_phrases)\n",
    "        # Yield the data in a X, Y form\n",
    "        yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a final generator holding all the context\n",
    "def training_generator():\n",
    "    gen = vectorized_gen(input_phrases, chunk_size,\n",
    "                         misspellings_count, noise,\n",
    "                         training_vectorizer)\n",
    "    yield from gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Real) Epoch: 0\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.4854 - val_loss: 0.4298\n",
      "Test Error: 0.975\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.3487 - val_loss: 0.3132\n",
      "Test Error: 0.9\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.2604 - val_loss: 0.2393\n",
      "Test Error: 0.84\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.2021 - val_loss: 0.1861\n",
      "Test Error: 0.73\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.1685 - val_loss: 0.1694\n",
      "Test Error: 0.74\n",
      "(Real) Epoch: 1\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.1413 - val_loss: 0.1461\n",
      "Test Error: 0.705\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.1263 - val_loss: 0.1403\n",
      "Test Error: 0.63\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.1144 - val_loss: 0.1236\n",
      "Test Error: 0.59\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.1035 - val_loss: 0.1147\n",
      "Test Error: 0.525\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0972 - val_loss: 0.1125\n",
      "Test Error: 0.54\n",
      "(Real) Epoch: 2\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0897 - val_loss: 0.1023\n",
      "Test Error: 0.5\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0845 - val_loss: 0.0926\n",
      "Test Error: 0.505\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0814 - val_loss: 0.0903\n",
      "Test Error: 0.52\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0770 - val_loss: 0.0877\n",
      "Test Error: 0.53\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0746 - val_loss: 0.0875\n",
      "Test Error: 0.49\n",
      "(Real) Epoch: 3\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0709 - val_loss: 0.0829\n",
      "Test Error: 0.475\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0688 - val_loss: 0.0787\n",
      "Test Error: 0.475\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0672 - val_loss: 0.0797\n",
      "Test Error: 0.405\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0645 - val_loss: 0.0747\n",
      "Test Error: 0.46\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0633 - val_loss: 0.0738\n",
      "Test Error: 0.415\n",
      "(Real) Epoch: 4\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0613 - val_loss: 0.0708\n",
      "Test Error: 0.44\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0601 - val_loss: 0.0711\n",
      "Test Error: 0.425\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0588 - val_loss: 0.0681\n",
      "Test Error: 0.385\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0570 - val_loss: 0.0688\n",
      "Test Error: 0.405\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0562 - val_loss: 0.0687\n",
      "Test Error: 0.405\n",
      "(Real) Epoch: 5\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0552 - val_loss: 0.0666\n",
      "Test Error: 0.365\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0538 - val_loss: 0.0643\n",
      "Test Error: 0.36\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0537 - val_loss: 0.0641\n",
      "Test Error: 0.365\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0523 - val_loss: 0.0618\n",
      "Test Error: 0.4\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0520 - val_loss: 0.0627\n",
      "Test Error: 0.4\n",
      "(Real) Epoch: 6\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0509 - val_loss: 0.0623\n",
      "Test Error: 0.375\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0501 - val_loss: 0.0596\n",
      "Test Error: 0.39\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0497 - val_loss: 0.0583\n",
      "Test Error: 0.355\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0487 - val_loss: 0.0587\n",
      "Test Error: 0.355\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0483 - val_loss: 0.0585\n",
      "Test Error: 0.405\n",
      "(Real) Epoch: 7\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0480 - val_loss: 0.0574\n",
      "Test Error: 0.325\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0470 - val_loss: 0.0566\n",
      "Test Error: 0.425\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0469 - val_loss: 0.0561\n",
      "Test Error: 0.325\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0455 - val_loss: 0.0558\n",
      "Test Error: 0.355\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0460 - val_loss: 0.0555\n",
      "Test Error: 0.395\n",
      "(Real) Epoch: 8\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0453 - val_loss: 0.0541\n",
      "Test Error: 0.345\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0448 - val_loss: 0.0540\n",
      "Test Error: 0.365\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0445 - val_loss: 0.0535\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0435 - val_loss: 0.0515\n",
      "Test Error: 0.345\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0438 - val_loss: 0.0550\n",
      "Test Error: 0.33\n",
      "(Real) Epoch: 9\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0432 - val_loss: 0.0529\n",
      "Test Error: 0.365\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0427 - val_loss: 0.0542\n",
      "Test Error: 0.41\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0425 - val_loss: 0.0511\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0415 - val_loss: 0.0514\n",
      "Test Error: 0.34\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0419 - val_loss: 0.0527\n",
      "Test Error: 0.325\n",
      "(Real) Epoch: 10\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0413 - val_loss: 0.0518\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0408 - val_loss: 0.0514\n",
      "Test Error: 0.36\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0409 - val_loss: 0.0498\n",
      "Test Error: 0.335\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0403 - val_loss: 0.0500\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0401 - val_loss: 0.0495\n",
      "Test Error: 0.33\n",
      "(Real) Epoch: 11\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0397 - val_loss: 0.0503\n",
      "Test Error: 0.34\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0393 - val_loss: 0.0490\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0392 - val_loss: 0.0492\n",
      "Test Error: 0.35\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0393 - val_loss: 0.0484\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0388 - val_loss: 0.0481\n",
      "Test Error: 0.36\n",
      "(Real) Epoch: 12\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0387 - val_loss: 0.0480\n",
      "Test Error: 0.335\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0382 - val_loss: 0.0465\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0383 - val_loss: 0.0461\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0377 - val_loss: 0.0474\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0376 - val_loss: 0.0473\n",
      "Test Error: 0.315\n",
      "(Real) Epoch: 13\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0373 - val_loss: 0.0472\n",
      "Test Error: 0.315\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0373 - val_loss: 0.0477\n",
      "Test Error: 0.345\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0369 - val_loss: 0.0455\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0365 - val_loss: 0.0452\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0369 - val_loss: 0.0455\n",
      "Test Error: 0.305\n",
      "(Real) Epoch: 14\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0364 - val_loss: 0.0475\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0361 - val_loss: 0.0455\n",
      "Test Error: 0.345\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0366 - val_loss: 0.0459\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0358 - val_loss: 0.0442\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0364 - val_loss: 0.0458\n",
      "Test Error: 0.35\n",
      "(Real) Epoch: 15\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0357 - val_loss: 0.0466\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0356 - val_loss: 0.0454\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0354 - val_loss: 0.0445\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0350 - val_loss: 0.0443\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0352 - val_loss: 0.0453\n",
      "Test Error: 0.305\n",
      "(Real) Epoch: 16\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0350 - val_loss: 0.0451\n",
      "Test Error: 0.335\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0347 - val_loss: 0.0429\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0346 - val_loss: 0.0427\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0342 - val_loss: 0.0442\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0344 - val_loss: 0.0426\n",
      "Test Error: 0.24\n",
      "(Real) Epoch: 17\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0346 - val_loss: 0.0441\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0339 - val_loss: 0.0439\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0342 - val_loss: 0.0417\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0338 - val_loss: 0.0422\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0340 - val_loss: 0.0434\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 18\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 184s 1ms/step - loss: 0.0336 - val_loss: 0.0429\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0333 - val_loss: 0.0417\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 184s 1ms/step - loss: 0.0334 - val_loss: 0.0423\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0328 - val_loss: 0.0418\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0333 - val_loss: 0.0423\n",
      "Test Error: 0.33\n",
      "(Real) Epoch: 19\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0333 - val_loss: 0.0422\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0329 - val_loss: 0.0414\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0329 - val_loss: 0.0410\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0328 - val_loss: 0.0405\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0330 - val_loss: 0.0418\n",
      "Test Error: 0.31\n",
      "(Real) Epoch: 20\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0325 - val_loss: 0.0412\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0323 - val_loss: 0.0418\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0325 - val_loss: 0.0403\n",
      "Test Error: 0.33\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0320 - val_loss: 0.0399\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0322 - val_loss: 0.0404\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 21\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0321 - val_loss: 0.0409\n",
      "Test Error: 0.335\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0319 - val_loss: 0.0415\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0323 - val_loss: 0.0399\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0317 - val_loss: 0.0411\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0318 - val_loss: 0.0406\n",
      "Test Error: 0.305\n",
      "(Real) Epoch: 22\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0315 - val_loss: 0.0392\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0312 - val_loss: 0.0405\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0317 - val_loss: 0.0409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 0.28\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0310 - val_loss: 0.0387\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0312 - val_loss: 0.0403\n",
      "Test Error: 0.315\n",
      "(Real) Epoch: 23\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0313 - val_loss: 0.0401\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0307 - val_loss: 0.0403\n",
      "Test Error: 0.34\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0311 - val_loss: 0.0392\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0308 - val_loss: 0.0390\n",
      "Test Error: 0.325\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0309 - val_loss: 0.0396\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 24\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0309 - val_loss: 0.0392\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0309 - val_loss: 0.0389\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0308 - val_loss: 0.0380\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0304 - val_loss: 0.0385\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0306 - val_loss: 0.0394\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 25\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0302 - val_loss: 0.0387\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0303 - val_loss: 0.0377\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0303 - val_loss: 0.0381\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0301 - val_loss: 0.0391\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0303 - val_loss: 0.0392\n",
      "Test Error: 0.275\n",
      "(Real) Epoch: 26\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0303 - val_loss: 0.0379\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0301 - val_loss: 0.0379\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0301 - val_loss: 0.0388\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0299 - val_loss: 0.0373\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0300 - val_loss: 0.0385\n",
      "Test Error: 0.295\n",
      "(Real) Epoch: 27\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0298 - val_loss: 0.0384\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0297 - val_loss: 0.0388\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0292 - val_loss: 0.0372\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0296 - val_loss: 0.0367\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0296 - val_loss: 0.0385\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 28\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0295 - val_loss: 0.0391\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0292 - val_loss: 0.0382\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0295 - val_loss: 0.0382\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0293 - val_loss: 0.0379\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0294 - val_loss: 0.0382\n",
      "Test Error: 0.34\n",
      "(Real) Epoch: 29\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0291 - val_loss: 0.0372\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0290 - val_loss: 0.0367\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0293 - val_loss: 0.0370\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0285 - val_loss: 0.0371\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0292 - val_loss: 0.0366\n",
      "Test Error: 0.26\n",
      "(Real) Epoch: 30\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0288 - val_loss: 0.0373\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0286 - val_loss: 0.0370\n",
      "Test Error: 0.34\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0289 - val_loss: 0.0370\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0286 - val_loss: 0.0361\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0286 - val_loss: 0.0370\n",
      "Test Error: 0.265\n",
      "(Real) Epoch: 31\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0285 - val_loss: 0.0370\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0285 - val_loss: 0.0367\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0286 - val_loss: 0.0361\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0283 - val_loss: 0.0370\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0286 - val_loss: 0.0358\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 32\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0284 - val_loss: 0.0365\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0282 - val_loss: 0.0363\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0283 - val_loss: 0.0358\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0282 - val_loss: 0.0348\n",
      "Test Error: 0.195\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0285 - val_loss: 0.0372\n",
      "Test Error: 0.3\n",
      "(Real) Epoch: 33\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0284 - val_loss: 0.0365\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0282 - val_loss: 0.0370\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0279 - val_loss: 0.0353\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0279 - val_loss: 0.0350\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0282 - val_loss: 0.0369\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 34\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0280 - val_loss: 0.0345\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0278 - val_loss: 0.0350\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0276 - val_loss: 0.0354\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0276 - val_loss: 0.0357\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0280 - val_loss: 0.0348\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 35\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0276 - val_loss: 0.0353\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0275 - val_loss: 0.0352\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0277 - val_loss: 0.0356\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0274 - val_loss: 0.0335\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0277 - val_loss: 0.0351\n",
      "Test Error: 0.285\n",
      "(Real) Epoch: 36\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0278 - val_loss: 0.0366\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0272 - val_loss: 0.0358\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0275 - val_loss: 0.0364\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0274 - val_loss: 0.0351\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0272 - val_loss: 0.0348\n",
      "Test Error: 0.265\n",
      "(Real) Epoch: 37\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0272 - val_loss: 0.0351\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0270 - val_loss: 0.0343\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0273 - val_loss: 0.0355\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #120000 to #160000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0271 - val_loss: 0.0342\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0273 - val_loss: 0.0346\n",
      "Test Error: 0.225\n",
      "(Real) Epoch: 38\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0275 - val_loss: 0.0351\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0270 - val_loss: 0.0343\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 194s 1ms/step - loss: 0.0274 - val_loss: 0.0352\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0271 - val_loss: 0.0344\n",
      "Test Error: 0.205\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0269 - val_loss: 0.0347\n",
      "Test Error: 0.335\n",
      "(Real) Epoch: 39\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0269 - val_loss: 0.0352\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0268 - val_loss: 0.0343\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0266 - val_loss: 0.0358\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0266 - val_loss: 0.0344\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0270 - val_loss: 0.0349\n",
      "Test Error: 0.245\n",
      "(Real) Epoch: 40\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0267 - val_loss: 0.0339\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 192s 1ms/step - loss: 0.0267 - val_loss: 0.0336\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0270 - val_loss: 0.0341\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0262 - val_loss: 0.0343\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0268 - val_loss: 0.0348\n",
      "Test Error: 0.285\n",
      "(Real) Epoch: 41\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0265 - val_loss: 0.0343\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0266 - val_loss: 0.0341\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0265 - val_loss: 0.0344\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0266 - val_loss: 0.0341\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0265 - val_loss: 0.0348\n",
      "Test Error: 0.29\n",
      "(Real) Epoch: 42\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0264 - val_loss: 0.0348\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0263 - val_loss: 0.0351\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0264 - val_loss: 0.0339\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0263 - val_loss: 0.0351\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0266 - val_loss: 0.0345\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 43\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0264 - val_loss: 0.0338\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0262 - val_loss: 0.0351\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0262 - val_loss: 0.0337\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0260 - val_loss: 0.0331\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0263 - val_loss: 0.0338\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 44\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0264 - val_loss: 0.0348\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0261 - val_loss: 0.0336\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0261 - val_loss: 0.0342\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0258 - val_loss: 0.0351\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0263 - val_loss: 0.0344\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 45\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0259 - val_loss: 0.0349\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0258 - val_loss: 0.0341\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0260 - val_loss: 0.0339\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0259 - val_loss: 0.0323\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0258 - val_loss: 0.0339\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 46\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0260 - val_loss: 0.0333\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0256 - val_loss: 0.0332\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0259 - val_loss: 0.0337\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0257 - val_loss: 0.0324\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0258 - val_loss: 0.0327\n",
      "Test Error: 0.22\n",
      "(Real) Epoch: 47\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0256 - val_loss: 0.0341\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0258 - val_loss: 0.0341\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0257 - val_loss: 0.0330\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 185s 1ms/step - loss: 0.0256 - val_loss: 0.0325\n",
      "Test Error: 0.2\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0254 - val_loss: 0.0344\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 48\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0255 - val_loss: 0.0333\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0255 - val_loss: 0.0336\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0255 - val_loss: 0.0332\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0254 - val_loss: 0.0323\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0256 - val_loss: 0.0330\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 49\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0254 - val_loss: 0.0332\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0255 - val_loss: 0.0344\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0256 - val_loss: 0.0323\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0253 - val_loss: 0.0326\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0255 - val_loss: 0.0330\n",
      "Test Error: 0.29\n",
      "(Real) Epoch: 50\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0252 - val_loss: 0.0333\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0256 - val_loss: 0.0321\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0253 - val_loss: 0.0330\n",
      "Test Error: 0.21\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0255 - val_loss: 0.0326\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0252 - val_loss: 0.0336\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 51\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0251 - val_loss: 0.0321\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0254 - val_loss: 0.0331\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0252 - val_loss: 0.0318\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0252 - val_loss: 0.0321\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0251 - val_loss: 0.0328\n",
      "Test Error: 0.29\n",
      "(Real) Epoch: 52\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0251 - val_loss: 0.0323\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0250 - val_loss: 0.0332\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0251 - val_loss: 0.0315\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #120000 to #160000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0249 - val_loss: 0.0328\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0252 - val_loss: 0.0327\n",
      "Test Error: 0.205\n",
      "(Real) Epoch: 53\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 184s 1ms/step - loss: 0.0253 - val_loss: 0.0323\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0248 - val_loss: 0.0330\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 187s 1ms/step - loss: 0.0250 - val_loss: 0.0327\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0247 - val_loss: 0.0321\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0249 - val_loss: 0.0322\n",
      "Test Error: 0.275\n",
      "(Real) Epoch: 54\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 184s 1ms/step - loss: 0.0248 - val_loss: 0.0323\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0248 - val_loss: 0.0321\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0249 - val_loss: 0.0321\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0247 - val_loss: 0.0316\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0251 - val_loss: 0.0330\n",
      "Test Error: 0.245\n",
      "(Real) Epoch: 55\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0247 - val_loss: 0.0332\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0250 - val_loss: 0.0321\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0249 - val_loss: 0.0334\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0245 - val_loss: 0.0322\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0249 - val_loss: 0.0325\n",
      "Test Error: 0.285\n",
      "(Real) Epoch: 56\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0248 - val_loss: 0.0320\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0244 - val_loss: 0.0323\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0245 - val_loss: 0.0334\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0247 - val_loss: 0.0313\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0248 - val_loss: 0.0323\n",
      "Test Error: 0.26\n",
      "(Real) Epoch: 57\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0246 - val_loss: 0.0319\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0246 - val_loss: 0.0325\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0245 - val_loss: 0.0321\n",
      "Test Error: 0.33\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0243 - val_loss: 0.0326\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0246 - val_loss: 0.0327\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 58\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0246 - val_loss: 0.0327\n",
      "Test Error: 0.21\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0243 - val_loss: 0.0329\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0242 - val_loss: 0.0326\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0245 - val_loss: 0.0319\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0245 - val_loss: 0.0312\n",
      "Test Error: 0.29\n",
      "(Real) Epoch: 59\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0243 - val_loss: 0.0316\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0244 - val_loss: 0.0322\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0241 - val_loss: 0.0318\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0244 - val_loss: 0.0315\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0243 - val_loss: 0.0319\n",
      "Test Error: 0.22\n",
      "(Real) Epoch: 60\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0241 - val_loss: 0.0317\n",
      "Test Error: 0.195\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0241 - val_loss: 0.0317\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0246 - val_loss: 0.0309\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0242 - val_loss: 0.0299\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0243 - val_loss: 0.0313\n",
      "Test Error: 0.245\n",
      "(Real) Epoch: 61\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0241 - val_loss: 0.0318\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0240 - val_loss: 0.0321\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0241 - val_loss: 0.0306\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0244 - val_loss: 0.0319\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0240 - val_loss: 0.0315\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 62\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0242 - val_loss: 0.0324\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0239 - val_loss: 0.0317\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0240 - val_loss: 0.0307\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0239 - val_loss: 0.0331\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0241 - val_loss: 0.0322\n",
      "Test Error: 0.26\n",
      "(Real) Epoch: 63\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0240 - val_loss: 0.0318\n",
      "Test Error: 0.22\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0239 - val_loss: 0.0307\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0241 - val_loss: 0.0315\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0242 - val_loss: 0.0304\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0243 - val_loss: 0.0306\n",
      "Test Error: 0.255\n",
      "(Real) Epoch: 64\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0240 - val_loss: 0.0316\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0236 - val_loss: 0.0322\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0240 - val_loss: 0.0310\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0238 - val_loss: 0.0312\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0239 - val_loss: 0.0326\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 65\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0237 - val_loss: 0.0315\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0238 - val_loss: 0.0310\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0239 - val_loss: 0.0310\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0239 - val_loss: 0.0304\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0238 - val_loss: 0.0322\n",
      "Test Error: 0.275\n",
      "(Real) Epoch: 66\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0241 - val_loss: 0.0316\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0238 - val_loss: 0.0307\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0239 - val_loss: 0.0303\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0238 - val_loss: 0.0306\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0238 - val_loss: 0.0303\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 67\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0237 - val_loss: 0.0315\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0237 - val_loss: 0.0313\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0236 - val_loss: 0.0310\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #120000 to #160000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0236 - val_loss: 0.0306\n",
      "Test Error: 0.22\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0237 - val_loss: 0.0310\n",
      "Test Error: 0.25\n",
      "(Real) Epoch: 68\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0239 - val_loss: 0.0308\n",
      "Test Error: 0.2\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0237 - val_loss: 0.0315\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0238 - val_loss: 0.0309\n",
      "Test Error: 0.305\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0234 - val_loss: 0.0306\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0237 - val_loss: 0.0318\n",
      "Test Error: 0.3\n",
      "(Real) Epoch: 69\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0235 - val_loss: 0.0303\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0237 - val_loss: 0.0312\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0235 - val_loss: 0.0315\n",
      "Test Error: 0.325\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0234 - val_loss: 0.0307\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0237 - val_loss: 0.0307\n",
      "Test Error: 0.215\n",
      "(Real) Epoch: 70\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0234 - val_loss: 0.0306\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0235 - val_loss: 0.0306\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0237 - val_loss: 0.0306\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0233 - val_loss: 0.0307\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0236 - val_loss: 0.0308\n",
      "Test Error: 0.205\n",
      "(Real) Epoch: 71\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0232 - val_loss: 0.0299\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0235 - val_loss: 0.0311\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0236 - val_loss: 0.0295\n",
      "Test Error: 0.22\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0232 - val_loss: 0.0306\n",
      "Test Error: 0.21\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0232 - val_loss: 0.0310\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 72\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0234 - val_loss: 0.0314\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0233 - val_loss: 0.0308\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0233 - val_loss: 0.0312\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0234 - val_loss: 0.0297\n",
      "Test Error: 0.21\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0236 - val_loss: 0.0312\n",
      "Test Error: 0.24\n",
      "(Real) Epoch: 73\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0233 - val_loss: 0.0301\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0234 - val_loss: 0.0305\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0235 - val_loss: 0.0313\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0231 - val_loss: 0.0299\n",
      "Test Error: 0.205\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0235 - val_loss: 0.0299\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 74\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0232 - val_loss: 0.0315\n",
      "Test Error: 0.17\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0233 - val_loss: 0.0304\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0233 - val_loss: 0.0297\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0230 - val_loss: 0.0303\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0232 - val_loss: 0.0305\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 75\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0230 - val_loss: 0.0313\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0233 - val_loss: 0.0307\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0230 - val_loss: 0.0305\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0231 - val_loss: 0.0301\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0232 - val_loss: 0.0300\n",
      "Test Error: 0.21\n",
      "(Real) Epoch: 76\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0232 - val_loss: 0.0306\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0233 - val_loss: 0.0292\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0232 - val_loss: 0.0301\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0232 - val_loss: 0.0311\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0231 - val_loss: 0.0314\n",
      "Test Error: 0.265\n",
      "(Real) Epoch: 77\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0229 - val_loss: 0.0308\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0231 - val_loss: 0.0303\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0228 - val_loss: 0.0291\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0233 - val_loss: 0.0295\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0230 - val_loss: 0.0304\n",
      "Test Error: 0.29\n",
      "(Real) Epoch: 78\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0231 - val_loss: 0.0302\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0227 - val_loss: 0.0309\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0233 - val_loss: 0.0301\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0231 - val_loss: 0.0297\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0232 - val_loss: 0.0302\n",
      "Test Error: 0.215\n",
      "(Real) Epoch: 79\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0229 - val_loss: 0.0302\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0229 - val_loss: 0.0300\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0230 - val_loss: 0.0296\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0227 - val_loss: 0.0290\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 193s 1ms/step - loss: 0.0228 - val_loss: 0.0303\n",
      "Test Error: 0.22\n",
      "(Real) Epoch: 80\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0230 - val_loss: 0.0297\n",
      "Test Error: 0.21\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0228 - val_loss: 0.0309\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "100352/144000 [===================>..........] - ETA: 46s - loss: 0.0232"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"(Real) Epoch: %s\" % epoch)\n",
    "    # Initialize the generator\n",
    "    gen = training_generator()\n",
    "    for X, Y in gen:\n",
    "        model.fit(X, Y,\n",
    "              batch_size=batch_size,epochs=1, validation_split=0.1)\n",
    "        print('Test Error:', evaluate_misspelled(test_phrases[:200], corrector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_correct(input_phrases[:1000], translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_correct(input_phrases[-1000:], translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_correct(test_phrases[:1000], translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_correct(test_phrases[:1000], translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vect(input_texts, target_texts, model, training_vectorizer):\n",
    "    target_texts = wrap_with_delims(target_texts)\n",
    "    \n",
    "    #wrapped_target_texts = wrap_with_delims(target_texts)\n",
    "    X, Y = training_vectorizer(input_texts, target_texts)\n",
    "    loss = model.evaluate(X, Y)\n",
    "    print('\\nTesting loss: ', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_vect(input_phrases[:1000], input_phrases[:1000], model, training_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tfrier\\n',\n",
       " '\\tstep\\n',\n",
       " '\\tcome in\\n',\n",
       " '\\tget ot\\n',\n",
       " \"\\ti can't go\\n\",\n",
       " \"\\ti'm sorry\\n\",\n",
       " '\\the is busi\\n',\n",
       " \"\\the's drunk\\n\",\n",
       " \"\\ti'll be late\\n\",\n",
       " '\\thold my beer\\n',\n",
       " '\\tput the butzn\\n',\n",
       " '\\tcoll me on my phone\\n',\n",
       " '\\thello boys and girls\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max encoder seq legth\n",
    "#max_encoder_seq_length = encoder_model.get_layer('encoder_inputs').input_shape[-1]\n",
    "phrases = ['fire', 'stp', 'comein', 'get ot', 'i cant go','im sorry', \n",
    "           'h is busi', 'hes drunk', 'ill be lat', 'hold mi beer', 'pus the buton', \n",
    "          'coll me on my phone', 'helo boys and girls']\n",
    "\n",
    "[translator(phrase) for phrase in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0cc3b6e4efb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisspelled_test_phrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_phrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_phrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-0cc3b6e4efb8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisspelled_test_phrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_phrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_phrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translator' is not defined"
     ]
    }
   ],
   "source": [
    "test_phrases = all_phrases[20:1020]\n",
    "misspelled_test_phrases = [add_noise_to_string(p, .1) for p in test_phrases]\n",
    "\n",
    "pairs = zip(misspelled_test_phrases, test_phrases)\n",
    "errors = list(filter(lambda p: cor(p[0]) != p[1], pairs))\n",
    "len(errors)/float(len(test_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    \"\"\"quick-n-dirty helper for saving models\"\"\"\n",
    "    print(\"Saving model\")\n",
    "    model.save('training.h5')\n",
    "    encoder_model.save('encoder.h5')\n",
    "    decoder_model.save('decoder.h5')\n",
    "\n",
    "    model_metadata = { 'input_token_index': input_token_index, \n",
    "                       'target_token_index': target_token_index,\n",
    "                       'max_encoder_seq_length': max_encoder_seq_length }\n",
    "\n",
    "    with open('model_metadata.pickle', 'wb') as f:\n",
    "        pickle.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i sure hope that tom doesn't sing\",\n",
       " 'i thought tom and mary were supposed to be here yesterday',\n",
       " \"i've been living in a cave\",\n",
       " 'the child is learning quickly',\n",
       " 'she turned off the lights so she could enjoy the moonlight',\n",
       " 'tom asked me not to do that anymore',\n",
       " 'tom had an allergic reaction to the medication',\n",
       " 'he was shorter than me',\n",
       " \"i didn't tell tom who told me that he'd stolen my saxophone\",\n",
       " \"there's no cause for concern\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_phrases[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
