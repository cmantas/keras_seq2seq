{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# ignore some Keras warnings regarding deprecations and model saving \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import pickle\n",
    "\n",
    "from helpers import *\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences from the [tatoeba dataset](https://tatoeba.org/eng/downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 20  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples =200000 # Number of samples to train on.\n",
    "# Path to the datatxt file on disk.\n",
    "data_path = 'sentences.txt'\n",
    "\n",
    "epochs = 100\n",
    "noise = 0.05\n",
    "misspellings_count = 3\n",
    "chunk_size = 40000\n",
    "\n",
    "optimizer= 'adam'\n",
    "loss_fn='categorical_crossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-pick maximum sequence lengths\n",
    "max_encoder_seq_length = 25\n",
    "max_decoder_seq_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed(data_path):\n",
    "    with open(data_path) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = text_preprocess(lines)\n",
    "    # allow only for a limited count of \n",
    "    allowed_chars = {'.', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', \n",
    "             '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', \n",
    "             '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', \n",
    "             '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    "             'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "             's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}' }\n",
    "    selected = []\n",
    "    for l in lines:\n",
    "        if all([c in allowed_chars for c in l.strip()]) and \\\n",
    "           len(l) < max_encoder_seq_length:\n",
    "            selected.append(l)\n",
    "    # suffle deterministically\n",
    "    Random(0).shuffle(selected)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All phrases in dataset:  210139\n",
      "Training phrases:  200000\n",
      "Test phrases:  10139\n",
      "Examples:\n",
      " * leave him alone, please\n",
      " * what do you call love?\n",
      " * tom got in the taxi\n",
      " * we can trust tom\n",
      " * i'd like you to meet tom\n",
      " * the money is terrible\n",
      " * did you go to see him?\n",
      " * monday will be a hot day\n",
      " * tom took mary to dinner\n",
      " * turn on the light please\n"
     ]
    }
   ],
   "source": [
    "all_phrases = load_preprocessed(data_path)\n",
    "input_phrases = all_phrases[:num_samples]\n",
    "test_phrases = all_phrases[num_samples:]\n",
    "print('All phrases in dataset: ', len(all_phrases))\n",
    "print('Training phrases: ', len(input_phrases))\n",
    "print('Test phrases: ', len(test_phrases))\n",
    "\n",
    "print(\"\\n * \".join(['Examples:'] + all_phrases[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique input tokens: 55\n",
      "Number of unique output tokens: 57\n"
     ]
    }
   ],
   "source": [
    "# create doken indices out of all phrases\n",
    "input_token_index = token_index(all_phrases)\n",
    "num_encoder_tokens = len(input_token_index)\n",
    "target_token_index = {'\\t': num_encoder_tokens, \n",
    "                      '\\n': num_encoder_tokens+1, \n",
    "                      **input_token_index}\n",
    "num_decoder_tokens = len(target_token_index)\n",
    "\n",
    "# Keep the count of all the possible input characters\n",
    "\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(num_encoder_tokens, num_decoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder')\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, \n",
    "                        name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', \n",
    "                          name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Next: inference mode (sampling).\n",
    "    # Here's the drill:\n",
    "    # 1) encode input and retrieve initial decoder state\n",
    "    # 2) run one step of decoder with this initial state\n",
    "    # and a \"start of sequence\" token as target.\n",
    "    # Output will be the next target token\n",
    "    # 3) Repeat with the current target token and current states\n",
    "\n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = models(num_encoder_tokens, num_decoder_tokens, latent_dim)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train few epochs on an identity fn with a chunk of the dataset for sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_phrases = input_phrases[:10000]\n",
    "X, Y = vectorize_dataset(train_input_phrases, wrap_with_delims(train_input_phrases),\n",
    "                  input_token_index,target_token_index,\n",
    "                  max_encoder_seq_length, max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 2.0636 - val_loss: 1.8943\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.8237 - val_loss: 1.7181\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.6187 - val_loss: 1.5311\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.4799 - val_loss: 1.4688\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.4185 - val_loss: 1.3774\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.3446 - val_loss: 1.3240\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.2901 - val_loss: 1.2637\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.2411 - val_loss: 1.2104\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.2033 - val_loss: 1.1754\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.1490 - val_loss: 1.1284\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.1134 - val_loss: 1.1491\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.0831 - val_loss: 1.0644\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.0462 - val_loss: 1.0379\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.0044 - val_loss: 0.9980\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 1.0007 - val_loss: 0.9688\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.9275 - val_loss: 0.9221\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.8868 - val_loss: 0.8692\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.8551 - val_loss: 0.8623\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.8189 - val_loss: 0.8099\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.7772 - val_loss: 0.7828\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.7639 - val_loss: 0.7641\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.7251 - val_loss: 0.7193\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.6952 - val_loss: 0.7163\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.6728 - val_loss: 0.6742\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.6416 - val_loss: 0.6825\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.6222 - val_loss: 0.6483\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.6067 - val_loss: 0.6362\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.5807 - val_loss: 0.6043\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.5623 - val_loss: 0.5994\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.5354 - val_loss: 0.5801\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.5174 - val_loss: 0.5566\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.5030 - val_loss: 0.5544\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.4879 - val_loss: 0.5698\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.4641 - val_loss: 0.5378\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.4498 - val_loss: 0.5163\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.4301 - val_loss: 0.5262\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.4228 - val_loss: 0.4918\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.4017 - val_loss: 0.4518\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3902 - val_loss: 0.4590\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3791 - val_loss: 0.4330\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3588 - val_loss: 0.4532\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3500 - val_loss: 0.4024\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3385 - val_loss: 0.4225\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3209 - val_loss: 0.3931\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3129 - val_loss: 0.3944\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.3020 - val_loss: 0.4150\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2966 - val_loss: 0.3802\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2722 - val_loss: 0.4187\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2752 - val_loss: 0.3720\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2876 - val_loss: 0.3471\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2494 - val_loss: 0.3317\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2386 - val_loss: 0.3480\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2372 - val_loss: 0.3224\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2323 - val_loss: 0.3390\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2198 - val_loss: 0.3108\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2087 - val_loss: 0.3060\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2048 - val_loss: 0.3237\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.2049 - val_loss: 0.3303\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1935 - val_loss: 0.2853\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1843 - val_loss: 0.2858\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1738 - val_loss: 0.2945\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1987 - val_loss: 0.2898\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1649 - val_loss: 0.2833\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1594 - val_loss: 0.2828\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1510 - val_loss: 0.2770\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1467 - val_loss: 0.2645\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1448 - val_loss: 0.2615\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1366 - val_loss: 0.2597\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1432 - val_loss: 0.2833\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1380 - val_loss: 0.2477\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1261 - val_loss: 0.2519\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1275 - val_loss: 0.2590\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1165 - val_loss: 0.2757\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1201 - val_loss: 0.2546\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1116 - val_loss: 0.2636\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1061 - val_loss: 0.3056\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1363 - val_loss: 0.2570\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1037 - val_loss: 0.2624\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0986 - val_loss: 0.2514\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0946 - val_loss: 0.2463\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0901 - val_loss: 0.2383\n",
      "Epoch 82/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0946 - val_loss: 0.2541\n",
      "Epoch 83/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0835 - val_loss: 0.2723\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0834 - val_loss: 0.2402\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0763 - val_loss: 0.2481\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0811 - val_loss: 0.2400\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0765 - val_loss: 0.2358\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0646 - val_loss: 0.2446\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0907 - val_loss: 0.3357\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0981 - val_loss: 0.2407\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0677 - val_loss: 0.2410\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0657 - val_loss: 0.2484\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0605 - val_loss: 0.2386\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0560 - val_loss: 0.2394\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0554 - val_loss: 0.2400\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0569 - val_loss: 0.2397\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0522 - val_loss: 0.2362\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0541 - val_loss: 0.2425\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0513 - val_loss: 0.2579\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0578 - val_loss: 0.2486\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8241669630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X,Y,\n",
    "                   batch_size=batch_size, epochs=epochs, \n",
    "                   validation_split=0.1)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"corrector\" fn that will \"translate\" our misspelled input to a right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector = translate_fn(encoder_model, decoder_model, \n",
    "                         input_token_index, target_token_index,\n",
    "                        max_encoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leave him alone pescead',\n",
       " 'what do you lacl ovel?',\n",
       " 'tom got in the taxt',\n",
       " 'we can trust tom',\n",
       " \"i'd like you to meet tom\",\n",
       " 'the money is terrible',\n",
       " 'did you go to see him?',\n",
       " 'monday will be a hot day',\n",
       " 'tom took mary to dinner',\n",
       " 'turn on the light please']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what our model has learned so far\n",
    "# by trying to \"correct\" some correct phrases\n",
    "[corrector(p) for p in train_input_phrases[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_correct(texts, corrector):\n",
    "    errors = 0.0\n",
    "    for t in texts:\n",
    "        if t != corrector(t): errors += 1\n",
    "    return errors / len(texts)\n",
    "\n",
    "def evaluate_misspelled(texts, corrector):\n",
    "    errors = 0.0\n",
    "    for t in texts:\n",
    "        errored = add_noise_to_string(t, 0.05)\n",
    "        if t != corrector(errored): errors += 1\n",
    "    return errors / len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_correct(train_input_phrases[:100], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_misspelled(train_input_phrases[:100], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_vectorizer_fn(input_token_index, max_encoder_seq_length,\n",
    "                           target_token_index, max_decoder_seq_length):\n",
    "    # create a closure fn that \"knows\" the token indices and seq lengths\n",
    "    def training_vectorizer(input_texts, target_texts):\n",
    "        encoder_input_data = vectorize_batch(input_texts, input_token_index,\n",
    "                                             max_encoder_seq_length)\n",
    "        decoder_input_data = vectorize_batch(target_texts, target_token_index,\n",
    "                                             max_decoder_seq_length)\n",
    "                # same as decoder input data, but offset by one\n",
    "        decoder_output_data = vectorize_batch(target_texts, target_token_index,\n",
    "                                              max_decoder_seq_length, True)\n",
    "        X = [encoder_input_data, decoder_input_data]\n",
    "        Y = decoder_output_data\n",
    "        return X, Y\n",
    "\n",
    "    return training_vectorizer\n",
    "\n",
    "# Create a training_vectorizer that only accepts input and target texts\n",
    "training_vectorizer = training_vectorizer_fn(input_token_index, max_encoder_seq_length,\n",
    "                                             target_token_index, max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_gen(phrases, batch_size, misspellings_count, noise):\n",
    "    \"\"\"Goes through the given phrases, in `batch_size` batches, generating \n",
    "    `misspellings_count` misspelling allongside them.\n",
    "    On each iteration it yields `batch_size`* (1+ misspellings_count) strings: \n",
    "    the original strings and the misspellings generated out of them\"\"\"\n",
    "    for i in range(0, len(phrases), batch_size):\n",
    "        frrom = i\n",
    "        to = i+batch_size\n",
    "        print(\"Yielding phrases from #%d to #%d\" % (frrom, to))\n",
    "        yield create_misspellings(phrases[frrom:to], noise, misspellings_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding phrases from #0 to #3\n",
      "leave him alone, please -> leave him alone, please\n",
      "what do you call love? -> what do you call love?\n",
      "tom got in the taxi -> tom got in the taxi\n",
      "leave him alone, please -> leave him alone, please\n",
      "what do yu call love? -> what do you call love?\n",
      "tom gtok in thebtaxi -> tom got in the taxi\n",
      "leave hm alone, please -> leave him alone, please\n",
      "wha tdo you call xloye? -> what do you call love?\n",
      "tom got in th qaxi -> tom got in the taxi\n"
     ]
    }
   ],
   "source": [
    "tst = batched_gen(input_phrases, 3, 2, 0.07)\n",
    "inp, trgt = next(tst)\n",
    "for i,t in zip(inp, trgt):\n",
    "    print(i, '->',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_gen(phrases, batch_size, misspellings_count, noise,\n",
    "                   training_vectorizer):\n",
    "    \"\"\"Creates vextorized batches of phrases (that are wrapped with delims)\"\"\"\n",
    "    # Create a generator of misspelled strings from the input phrases\n",
    "    gen = batched_gen(phrases, batch_size, misspellings_count, noise)\n",
    "    \n",
    "    # Go through all the input phrases, generatiing misspellings, vectorizing them\n",
    "    # and yielding each batch\n",
    "    for input_phrases, target_phrases in gen:\n",
    "        target_phrases = wrap_with_delims(target_phrases)\n",
    "        X, Y = training_vectorizer(input_phrases, target_phrases)\n",
    "        # Yield the data in a X, Y form\n",
    "        yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a final generator holding all the context\n",
    "def training_generator():\n",
    "    gen = vectorized_gen(input_phrases, chunk_size,\n",
    "                         misspellings_count, noise,\n",
    "                         training_vectorizer)\n",
    "    yield from gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Real) Epoch: 0\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.2883 - val_loss: 0.2838\n",
      "Test Error: 0.822\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.2258 - val_loss: 0.2262\n",
      "Test Error: 0.78\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.1873 - val_loss: 0.1967\n",
      "Test Error: 0.727\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.1585 - val_loss: 0.1702\n",
      "Test Error: 0.663\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.1403 - val_loss: 0.1523\n",
      "Test Error: 0.649\n",
      "(Real) Epoch: 1\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.1235 - val_loss: 0.1371\n",
      "Test Error: 0.599\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.1137 - val_loss: 0.1265\n",
      "Test Error: 0.563\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.1043 - val_loss: 0.1151\n",
      "Test Error: 0.553\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0973 - val_loss: 0.1187\n",
      "Test Error: 0.557\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0922 - val_loss: 0.1062\n",
      "Test Error: 0.55\n",
      "(Real) Epoch: 2\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0866 - val_loss: 0.1013\n",
      "Test Error: 0.513\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0824 - val_loss: 0.0996\n",
      "Test Error: 0.511\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0787 - val_loss: 0.0906\n",
      "Test Error: 0.485\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0751 - val_loss: 0.0910\n",
      "Test Error: 0.502\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0730 - val_loss: 0.0854\n",
      "Test Error: 0.45\n",
      "(Real) Epoch: 3\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0696 - val_loss: 0.0840\n",
      "Test Error: 0.475\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0680 - val_loss: 0.0822\n",
      "Test Error: 0.44\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0657 - val_loss: 0.0795\n",
      "Test Error: 0.446\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0634 - val_loss: 0.0737\n",
      "Test Error: 0.427\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0627 - val_loss: 0.0753\n",
      "Test Error: 0.451\n",
      "(Real) Epoch: 4\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0606 - val_loss: 0.0738\n",
      "Test Error: 0.433\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0596 - val_loss: 0.0714\n",
      "Test Error: 0.442\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0576 - val_loss: 0.0692\n",
      "Test Error: 0.402\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0567 - val_loss: 0.0683\n",
      "Test Error: 0.419\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0558 - val_loss: 0.0672\n",
      "Test Error: 0.41\n",
      "(Real) Epoch: 5\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0545 - val_loss: 0.0672\n",
      "Test Error: 0.377\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0537 - val_loss: 0.0657\n",
      "Test Error: 0.398\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0524 - val_loss: 0.0649\n",
      "Test Error: 0.399\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0511 - val_loss: 0.0613\n",
      "Test Error: 0.375\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0510 - val_loss: 0.0604\n",
      "Test Error: 0.399\n",
      "(Real) Epoch: 6\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0499 - val_loss: 0.0617\n",
      "Test Error: 0.376\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0491 - val_loss: 0.0612\n",
      "Test Error: 0.391\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0487 - val_loss: 0.0599\n",
      "Test Error: 0.357\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0477 - val_loss: 0.0592\n",
      "Test Error: 0.378\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0475 - val_loss: 0.0576\n",
      "Test Error: 0.33\n",
      "(Real) Epoch: 7\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0467 - val_loss: 0.0566\n",
      "Test Error: 0.351\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0464 - val_loss: 0.0561\n",
      "Test Error: 0.359\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0456 - val_loss: 0.0573\n",
      "Test Error: 0.363\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0448 - val_loss: 0.0554\n",
      "Test Error: 0.346\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0447 - val_loss: 0.0561\n",
      "Test Error: 0.353\n",
      "(Real) Epoch: 8\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0439 - val_loss: 0.0553\n",
      "Test Error: 0.353\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0438 - val_loss: 0.0532\n",
      "Test Error: 0.37\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0429 - val_loss: 0.0522\n",
      "Test Error: 0.344\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0425 - val_loss: 0.0523\n",
      "Test Error: 0.345\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0429 - val_loss: 0.0539\n",
      "Test Error: 0.358\n",
      "(Real) Epoch: 9\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0421 - val_loss: 0.0524\n",
      "Test Error: 0.333\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0415 - val_loss: 0.0520\n",
      "Test Error: 0.353\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0415 - val_loss: 0.0492\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0408 - val_loss: 0.0515\n",
      "Test Error: 0.32\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0406 - val_loss: 0.0500\n",
      "Test Error: 0.322\n",
      "(Real) Epoch: 10\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0401 - val_loss: 0.0492\n",
      "Test Error: 0.329\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0400 - val_loss: 0.0490\n",
      "Test Error: 0.339\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0397 - val_loss: 0.0481\n",
      "Test Error: 0.327\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0392 - val_loss: 0.0494\n",
      "Test Error: 0.332\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0392 - val_loss: 0.0479\n",
      "Test Error: 0.315\n",
      "(Real) Epoch: 11\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0389 - val_loss: 0.0509\n",
      "Test Error: 0.31\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0385 - val_loss: 0.0489\n",
      "Test Error: 0.321\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0379 - val_loss: 0.0476\n",
      "Test Error: 0.323\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0379 - val_loss: 0.0454\n",
      "Test Error: 0.333\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0381 - val_loss: 0.0476\n",
      "Test Error: 0.318\n",
      "(Real) Epoch: 12\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0375 - val_loss: 0.0467\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0374 - val_loss: 0.0468\n",
      "Test Error: 0.337\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0367 - val_loss: 0.0469\n",
      "Test Error: 0.308\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0364 - val_loss: 0.0463\n",
      "Test Error: 0.334\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0369 - val_loss: 0.0464\n",
      "Test Error: 0.309\n",
      "(Real) Epoch: 13\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0363 - val_loss: 0.0458\n",
      "Test Error: 0.307\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0358 - val_loss: 0.0455\n",
      "Test Error: 0.313\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0358 - val_loss: 0.0459\n",
      "Test Error: 0.313\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0355 - val_loss: 0.0457\n",
      "Test Error: 0.332\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0356 - val_loss: 0.0457\n",
      "Test Error: 0.296\n",
      "(Real) Epoch: 14\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0355 - val_loss: 0.0444\n",
      "Test Error: 0.293\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0349 - val_loss: 0.0444\n",
      "Test Error: 0.322\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0348 - val_loss: 0.0436\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0346 - val_loss: 0.0442\n",
      "Test Error: 0.288\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0348 - val_loss: 0.0455\n",
      "Test Error: 0.293\n",
      "(Real) Epoch: 15\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0344 - val_loss: 0.0434\n",
      "Test Error: 0.302\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0344 - val_loss: 0.0448\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0341 - val_loss: 0.0446\n",
      "Test Error: 0.306\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0340 - val_loss: 0.0434\n",
      "Test Error: 0.319\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0340 - val_loss: 0.0439\n",
      "Test Error: 0.293\n",
      "(Real) Epoch: 16\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0338 - val_loss: 0.0418\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0338 - val_loss: 0.0432\n",
      "Test Error: 0.312\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0335 - val_loss: 0.0423\n",
      "Test Error: 0.309\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0331 - val_loss: 0.0410\n",
      "Test Error: 0.315\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0334 - val_loss: 0.0418\n",
      "Test Error: 0.292\n",
      "(Real) Epoch: 17\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0330 - val_loss: 0.0424\n",
      "Test Error: 0.295\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0330 - val_loss: 0.0425\n",
      "Test Error: 0.292\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0325 - val_loss: 0.0401\n",
      "Test Error: 0.286\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0325 - val_loss: 0.0407\n",
      "Test Error: 0.296\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0329 - val_loss: 0.0422\n",
      "Test Error: 0.304\n",
      "(Real) Epoch: 18\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0322 - val_loss: 0.0414\n",
      "Test Error: 0.307\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0322 - val_loss: 0.0410\n",
      "Test Error: 0.281\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 193s 1ms/step - loss: 0.0321 - val_loss: 0.0410\n",
      "Test Error: 0.29\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0316 - val_loss: 0.0403\n",
      "Test Error: 0.302\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0321 - val_loss: 0.0396\n",
      "Test Error: 0.278\n",
      "(Real) Epoch: 19\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0319 - val_loss: 0.0403\n",
      "Test Error: 0.317\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0319 - val_loss: 0.0401\n",
      "Test Error: 0.3\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0314 - val_loss: 0.0399\n",
      "Test Error: 0.288\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0315 - val_loss: 0.0402\n",
      "Test Error: 0.293\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0313 - val_loss: 0.0400\n",
      "Test Error: 0.27\n",
      "(Real) Epoch: 20\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0311 - val_loss: 0.0402\n",
      "Test Error: 0.312\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0313 - val_loss: 0.0397\n",
      "Test Error: 0.297\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0311 - val_loss: 0.0399\n",
      "Test Error: 0.286\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0311 - val_loss: 0.0394\n",
      "Test Error: 0.273\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0311 - val_loss: 0.0395\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 21\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0306 - val_loss: 0.0379\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0306 - val_loss: 0.0392\n",
      "Test Error: 0.284\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0306 - val_loss: 0.0387\n",
      "Test Error: 0.274\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0303 - val_loss: 0.0388\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0303 - val_loss: 0.0388\n",
      "Test Error: 0.289\n",
      "(Real) Epoch: 22\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0304 - val_loss: 0.0394\n",
      "Test Error: 0.303\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0302 - val_loss: 0.0397\n",
      "Test Error: 0.293\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0302 - val_loss: 0.0374\n",
      "Test Error: 0.291\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0298 - val_loss: 0.0383\n",
      "Test Error: 0.257\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0297 - val_loss: 0.0381\n",
      "Test Error: 0.269\n",
      "(Real) Epoch: 23\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0298 - val_loss: 0.0388\n",
      "Test Error: 0.278\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0299 - val_loss: 0.0396\n",
      "Test Error: 0.286\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0297 - val_loss: 0.0376\n",
      "Test Error: 0.296\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0295 - val_loss: 0.0384\n",
      "Test Error: 0.279\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0295 - val_loss: 0.0380\n",
      "Test Error: 0.278\n",
      "(Real) Epoch: 24\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0295 - val_loss: 0.0372\n",
      "Test Error: 0.286\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0293 - val_loss: 0.0386\n",
      "Test Error: 0.278\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0294 - val_loss: 0.0374\n",
      "Test Error: 0.272\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0292 - val_loss: 0.0377\n",
      "Test Error: 0.272\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0295 - val_loss: 0.0375\n",
      "Test Error: 0.278\n",
      "(Real) Epoch: 25\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0292 - val_loss: 0.0364\n",
      "Test Error: 0.276\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0287 - val_loss: 0.0373\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 194s 1ms/step - loss: 0.0288 - val_loss: 0.0366\n",
      "Test Error: 0.276\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 194s 1ms/step - loss: 0.0287 - val_loss: 0.0364\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0290 - val_loss: 0.0375\n",
      "Test Error: 0.26\n",
      "(Real) Epoch: 26\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0285 - val_loss: 0.0384\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0286 - val_loss: 0.0367\n",
      "Test Error: 0.278\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0285 - val_loss: 0.0359\n",
      "Test Error: 0.249\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0285 - val_loss: 0.0358\n",
      "Test Error: 0.283\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0286 - val_loss: 0.0369\n",
      "Test Error: 0.278\n",
      "(Real) Epoch: 27\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0283 - val_loss: 0.0367\n",
      "Test Error: 0.278\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0283 - val_loss: 0.0369\n",
      "Test Error: 0.274\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0283 - val_loss: 0.0352\n",
      "Test Error: 0.28\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0284 - val_loss: 0.0370\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0282 - val_loss: 0.0357\n",
      "Test Error: 0.272\n",
      "(Real) Epoch: 28\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0281 - val_loss: 0.0367\n",
      "Test Error: 0.272\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0281 - val_loss: 0.0369\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0285 - val_loss: 0.0354\n",
      "Test Error: 0.282\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0277 - val_loss: 0.0353\n",
      "Test Error: 0.291\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0278 - val_loss: 0.0352\n",
      "Test Error: 0.277\n",
      "(Real) Epoch: 29\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0278 - val_loss: 0.0358\n",
      "Test Error: 0.267\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0277 - val_loss: 0.0341\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0278 - val_loss: 0.0354\n",
      "Test Error: 0.274\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0275 - val_loss: 0.0342\n",
      "Test Error: 0.264\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0277 - val_loss: 0.0359\n",
      "Test Error: 0.267\n",
      "(Real) Epoch: 30\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0275 - val_loss: 0.0351\n",
      "Test Error: 0.286\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0273 - val_loss: 0.0355\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0273 - val_loss: 0.0360\n",
      "Test Error: 0.274\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0274 - val_loss: 0.0354\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0276 - val_loss: 0.0347\n",
      "Test Error: 0.261\n",
      "(Real) Epoch: 31\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 187s 1ms/step - loss: 0.0272 - val_loss: 0.0349\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0268 - val_loss: 0.0348\n",
      "Test Error: 0.258\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0273 - val_loss: 0.0338\n",
      "Test Error: 0.247\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0270 - val_loss: 0.0345\n",
      "Test Error: 0.285\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0272 - val_loss: 0.0362\n",
      "Test Error: 0.252\n",
      "(Real) Epoch: 32\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0272 - val_loss: 0.0357\n",
      "Test Error: 0.262\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0270 - val_loss: 0.0345\n",
      "Test Error: 0.274\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0267 - val_loss: 0.0348\n",
      "Test Error: 0.273\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0268 - val_loss: 0.0343\n",
      "Test Error: 0.282\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0271 - val_loss: 0.0344\n",
      "Test Error: 0.281\n",
      "(Real) Epoch: 33\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0267 - val_loss: 0.0347\n",
      "Test Error: 0.255\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0267 - val_loss: 0.0353\n",
      "Test Error: 0.281\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0268 - val_loss: 0.0334\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0263 - val_loss: 0.0345\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0267 - val_loss: 0.0344\n",
      "Test Error: 0.268\n",
      "(Real) Epoch: 34\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0267 - val_loss: 0.0345\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0264 - val_loss: 0.0352\n",
      "Test Error: 0.277\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0266 - val_loss: 0.0342\n",
      "Test Error: 0.266\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0266 - val_loss: 0.0341\n",
      "Test Error: 0.252\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0264 - val_loss: 0.0344\n",
      "Test Error: 0.271\n",
      "(Real) Epoch: 35\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 191s 1ms/step - loss: 0.0258 - val_loss: 0.0343\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0263 - val_loss: 0.0343\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0262 - val_loss: 0.0343\n",
      "Test Error: 0.271\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0262 - val_loss: 0.0342\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0264 - val_loss: 0.0346\n",
      "Test Error: 0.275\n",
      "(Real) Epoch: 36\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0262 - val_loss: 0.0342\n",
      "Test Error: 0.272\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0261 - val_loss: 0.0330\n",
      "Test Error: 0.293\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0258 - val_loss: 0.0323\n",
      "Test Error: 0.253\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0259 - val_loss: 0.0332\n",
      "Test Error: 0.282\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0262 - val_loss: 0.0335\n",
      "Test Error: 0.247\n",
      "(Real) Epoch: 37\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0262 - val_loss: 0.0338\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0257 - val_loss: 0.0344\n",
      "Test Error: 0.258\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0259 - val_loss: 0.0321\n",
      "Test Error: 0.247\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0257 - val_loss: 0.0321\n",
      "Test Error: 0.269\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0259 - val_loss: 0.0337\n",
      "Test Error: 0.264\n",
      "(Real) Epoch: 38\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0257 - val_loss: 0.0333\n",
      "Test Error: 0.257\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0257 - val_loss: 0.0349\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0254 - val_loss: 0.0326\n",
      "Test Error: 0.258\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0253 - val_loss: 0.0331\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0258 - val_loss: 0.0346\n",
      "Test Error: 0.268\n",
      "(Real) Epoch: 39\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0254 - val_loss: 0.0334\n",
      "Test Error: 0.262\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0256 - val_loss: 0.0340\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0256 - val_loss: 0.0319\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0255 - val_loss: 0.0317\n",
      "Test Error: 0.247\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0254 - val_loss: 0.0326\n",
      "Test Error: 0.262\n",
      "(Real) Epoch: 40\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0254 - val_loss: 0.0334\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0253 - val_loss: 0.0328\n",
      "Test Error: 0.284\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0253 - val_loss: 0.0326\n",
      "Test Error: 0.267\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0254 - val_loss: 0.0321\n",
      "Test Error: 0.252\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0254 - val_loss: 0.0324\n",
      "Test Error: 0.255\n",
      "(Real) Epoch: 41\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0252 - val_loss: 0.0326\n",
      "Test Error: 0.281\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0251 - val_loss: 0.0334\n",
      "Test Error: 0.269\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0252 - val_loss: 0.0322\n",
      "Test Error: 0.228\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0249 - val_loss: 0.0324\n",
      "Test Error: 0.266\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0253 - val_loss: 0.0332\n",
      "Test Error: 0.244\n",
      "(Real) Epoch: 42\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0249 - val_loss: 0.0327\n",
      "Test Error: 0.277\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0248 - val_loss: 0.0323\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0251 - val_loss: 0.0312\n",
      "Test Error: 0.252\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0252 - val_loss: 0.0319\n",
      "Test Error: 0.266\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0251 - val_loss: 0.0326\n",
      "Test Error: 0.252\n",
      "(Real) Epoch: 43\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0253 - val_loss: 0.0337\n",
      "Test Error: 0.267\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0248 - val_loss: 0.0337\n",
      "Test Error: 0.273\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0249 - val_loss: 0.0315\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0247 - val_loss: 0.0311\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0245 - val_loss: 0.0330\n",
      "Test Error: 0.264\n",
      "(Real) Epoch: 44\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0247 - val_loss: 0.0328\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0247 - val_loss: 0.0336\n",
      "Test Error: 0.266\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0249 - val_loss: 0.0317\n",
      "Test Error: 0.259\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0244 - val_loss: 0.0318\n",
      "Test Error: 0.264\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0244 - val_loss: 0.0321\n",
      "Test Error: 0.237\n",
      "(Real) Epoch: 45\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0247 - val_loss: 0.0321\n",
      "Test Error: 0.231\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0246 - val_loss: 0.0322\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0245 - val_loss: 0.0314\n",
      "Test Error: 0.278\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0243 - val_loss: 0.0318\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0246 - val_loss: 0.0320\n",
      "Test Error: 0.28\n",
      "(Real) Epoch: 46\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0245 - val_loss: 0.0314\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0242 - val_loss: 0.0316\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0246 - val_loss: 0.0320\n",
      "Test Error: 0.277\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0241 - val_loss: 0.0319\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0244 - val_loss: 0.0325\n",
      "Test Error: 0.249\n",
      "(Real) Epoch: 47\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0245 - val_loss: 0.0323\n",
      "Test Error: 0.253\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0243 - val_loss: 0.0313\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0244 - val_loss: 0.0315\n",
      "Test Error: 0.266\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0242 - val_loss: 0.0306\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0243 - val_loss: 0.0317\n",
      "Test Error: 0.259\n",
      "(Real) Epoch: 48\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0242 - val_loss: 0.0317\n",
      "Test Error: 0.228\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0241 - val_loss: 0.0321\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0242 - val_loss: 0.0312\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0241 - val_loss: 0.0301\n",
      "Test Error: 0.268\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0243 - val_loss: 0.0317\n",
      "Test Error: 0.251\n",
      "(Real) Epoch: 49\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0240 - val_loss: 0.0324\n",
      "Test Error: 0.253\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0239 - val_loss: 0.0310\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0239 - val_loss: 0.0309\n",
      "Test Error: 0.262\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0238 - val_loss: 0.0306\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0240 - val_loss: 0.0313\n",
      "Test Error: 0.237\n",
      "(Real) Epoch: 50\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0237 - val_loss: 0.0314\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0240 - val_loss: 0.0313\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0240 - val_loss: 0.0314\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0238 - val_loss: 0.0318\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0239 - val_loss: 0.0317\n",
      "Test Error: 0.269\n",
      "(Real) Epoch: 51\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0238 - val_loss: 0.0321\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0236 - val_loss: 0.0318\n",
      "Test Error: 0.249\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0240 - val_loss: 0.0314\n",
      "Test Error: 0.243\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0237 - val_loss: 0.0300\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0236 - val_loss: 0.0316\n",
      "Test Error: 0.26\n",
      "(Real) Epoch: 52\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0236 - val_loss: 0.0312\n",
      "Test Error: 0.275\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0232 - val_loss: 0.0317\n",
      "Test Error: 0.258\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 187s 1ms/step - loss: 0.0239 - val_loss: 0.0300\n",
      "Test Error: 0.223\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0236 - val_loss: 0.0308\n",
      "Test Error: 0.259\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0235 - val_loss: 0.0311\n",
      "Test Error: 0.247\n",
      "(Real) Epoch: 53\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0233 - val_loss: 0.0309\n",
      "Test Error: 0.241\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0232 - val_loss: 0.0314\n",
      "Test Error: 0.257\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0236 - val_loss: 0.0307\n",
      "Test Error: 0.253\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0234 - val_loss: 0.0308\n",
      "Test Error: 0.268\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0235 - val_loss: 0.0306\n",
      "Test Error: 0.24\n",
      "(Real) Epoch: 54\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0235 - val_loss: 0.0309\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0234 - val_loss: 0.0306\n",
      "Test Error: 0.247\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0235 - val_loss: 0.0304\n",
      "Test Error: 0.241\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0231 - val_loss: 0.0308\n",
      "Test Error: 0.243\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0234 - val_loss: 0.0316\n",
      "Test Error: 0.253\n",
      "(Real) Epoch: 55\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0234 - val_loss: 0.0310\n",
      "Test Error: 0.259\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0234 - val_loss: 0.0304\n",
      "Test Error: 0.27\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0234 - val_loss: 0.0296\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0233 - val_loss: 0.0310\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0235 - val_loss: 0.0306\n",
      "Test Error: 0.251\n",
      "(Real) Epoch: 56\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0231 - val_loss: 0.0293\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0234 - val_loss: 0.0304\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0233 - val_loss: 0.0291\n",
      "Test Error: 0.258\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0233 - val_loss: 0.0295\n",
      "Test Error: 0.264\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0231 - val_loss: 0.0302\n",
      "Test Error: 0.207\n",
      "(Real) Epoch: 57\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0230 - val_loss: 0.0318\n",
      "Test Error: 0.249\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0229 - val_loss: 0.0306\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0229 - val_loss: 0.0299\n",
      "Test Error: 0.251\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0228 - val_loss: 0.0297\n",
      "Test Error: 0.257\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0232 - val_loss: 0.0303\n",
      "Test Error: 0.237\n",
      "(Real) Epoch: 58\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0230 - val_loss: 0.0310\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0232 - val_loss: 0.0313\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0226 - val_loss: 0.0308\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0227 - val_loss: 0.0301\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0231 - val_loss: 0.0300\n",
      "Test Error: 0.259\n",
      "(Real) Epoch: 59\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0233 - val_loss: 0.0300\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 158s 1ms/step - loss: 0.0230 - val_loss: 0.0299\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 157s 1ms/step - loss: 0.0229 - val_loss: 0.0296\n",
      "Test Error: 0.231\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0230 - val_loss: 0.0300\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0230 - val_loss: 0.0298\n",
      "Test Error: 0.227\n",
      "(Real) Epoch: 60\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0228 - val_loss: 0.0296\n",
      "Test Error: 0.241\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0228 - val_loss: 0.0300\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0228 - val_loss: 0.0300\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0226 - val_loss: 0.0312\n",
      "Test Error: 0.227\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0228 - val_loss: 0.0309\n",
      "Test Error: 0.241\n",
      "(Real) Epoch: 61\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 156s 1ms/step - loss: 0.0228 - val_loss: 0.0304\n",
      "Test Error: 0.243\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0227 - val_loss: 0.0301\n",
      "Test Error: 0.257\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0229 - val_loss: 0.0296\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0226 - val_loss: 0.0288\n",
      "Test Error: 0.262\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0227 - val_loss: 0.0294\n",
      "Test Error: 0.246\n",
      "(Real) Epoch: 62\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0226 - val_loss: 0.0298\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0226 - val_loss: 0.0298\n",
      "Test Error: 0.234\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0228 - val_loss: 0.0296\n",
      "Test Error: 0.265\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0226 - val_loss: 0.0298\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0228 - val_loss: 0.0305\n",
      "Test Error: 0.233\n",
      "(Real) Epoch: 63\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0225 - val_loss: 0.0287\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0225 - val_loss: 0.0295\n",
      "Test Error: 0.223\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0226 - val_loss: 0.0294\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0227 - val_loss: 0.0296\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0223 - val_loss: 0.0295\n",
      "Test Error: 0.27\n",
      "(Real) Epoch: 64\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 185s 1ms/step - loss: 0.0227 - val_loss: 0.0300\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0226 - val_loss: 0.0288\n",
      "Test Error: 0.266\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0227 - val_loss: 0.0291\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0224 - val_loss: 0.0290\n",
      "Test Error: 0.233\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0223 - val_loss: 0.0298\n",
      "Test Error: 0.223\n",
      "(Real) Epoch: 65\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0226 - val_loss: 0.0298\n",
      "Test Error: 0.231\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0223 - val_loss: 0.0299\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0224 - val_loss: 0.0292\n",
      "Test Error: 0.249\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0223 - val_loss: 0.0300\n",
      "Test Error: 0.237\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0222 - val_loss: 0.0303\n",
      "Test Error: 0.236\n",
      "(Real) Epoch: 66\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0222 - val_loss: 0.0293\n",
      "Test Error: 0.221\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 166s 1ms/step - loss: 0.0225 - val_loss: 0.0293\n",
      "Test Error: 0.257\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0222 - val_loss: 0.0300\n",
      "Test Error: 0.243\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0222 - val_loss: 0.0303\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0223 - val_loss: 0.0299\n",
      "Test Error: 0.236\n",
      "(Real) Epoch: 67\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0222 - val_loss: 0.0293\n",
      "Test Error: 0.239\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0224 - val_loss: 0.0305\n",
      "Test Error: 0.247\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0221 - val_loss: 0.0286\n",
      "Test Error: 0.251\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0221 - val_loss: 0.0289\n",
      "Test Error: 0.239\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0222 - val_loss: 0.0294\n",
      "Test Error: 0.237\n",
      "(Real) Epoch: 68\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 183s 1ms/step - loss: 0.0224 - val_loss: 0.0284\n",
      "Test Error: 0.225\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0220 - val_loss: 0.0298\n",
      "Test Error: 0.234\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 193s 1ms/step - loss: 0.0222 - val_loss: 0.0286\n",
      "Test Error: 0.234\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0221 - val_loss: 0.0283\n",
      "Test Error: 0.241\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0222 - val_loss: 0.0293\n",
      "Test Error: 0.251\n",
      "(Real) Epoch: 69\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0220 - val_loss: 0.0300\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0218 - val_loss: 0.0293\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0222 - val_loss: 0.0294\n",
      "Test Error: 0.244\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0218 - val_loss: 0.0292\n",
      "Test Error: 0.233\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0219 - val_loss: 0.0288\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 70\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0220 - val_loss: 0.0291\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0221 - val_loss: 0.0284\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0222 - val_loss: 0.0284\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0218 - val_loss: 0.0290\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0220 - val_loss: 0.0301\n",
      "Test Error: 0.264\n",
      "(Real) Epoch: 71\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0219 - val_loss: 0.0295\n",
      "Test Error: 0.229\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0220 - val_loss: 0.0288\n",
      "Test Error: 0.237\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0218 - val_loss: 0.0284\n",
      "Test Error: 0.228\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0216 - val_loss: 0.0293\n",
      "Test Error: 0.254\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0219 - val_loss: 0.0292\n",
      "Test Error: 0.249\n",
      "(Real) Epoch: 72\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0218 - val_loss: 0.0283\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0218 - val_loss: 0.0292\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0217 - val_loss: 0.0275\n",
      "Test Error: 0.227\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0218 - val_loss: 0.0283\n",
      "Test Error: 0.228\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0218 - val_loss: 0.0281\n",
      "Test Error: 0.236\n",
      "(Real) Epoch: 73\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0219 - val_loss: 0.0280\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0218 - val_loss: 0.0277\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0217 - val_loss: 0.0285\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0217 - val_loss: 0.0286\n",
      "Test Error: 0.249\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0215 - val_loss: 0.0274\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 74\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 163s 1ms/step - loss: 0.0218 - val_loss: 0.0289\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0216 - val_loss: 0.0287\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0215 - val_loss: 0.0284\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0216 - val_loss: 0.0290\n",
      "Test Error: 0.256\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0216 - val_loss: 0.0281\n",
      "Test Error: 0.242\n",
      "(Real) Epoch: 75\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0218 - val_loss: 0.0288\n",
      "Test Error: 0.217\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0217 - val_loss: 0.0285\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0216 - val_loss: 0.0285\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0216 - val_loss: 0.0284\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0218 - val_loss: 0.0281\n",
      "Test Error: 0.241\n",
      "(Real) Epoch: 76\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0218 - val_loss: 0.0279\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0211 - val_loss: 0.0284\n",
      "Test Error: 0.262\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0216 - val_loss: 0.0279\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0215 - val_loss: 0.0272\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0217 - val_loss: 0.0285\n",
      "Test Error: 0.245\n",
      "(Real) Epoch: 77\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0219 - val_loss: 0.0284\n",
      "Test Error: 0.245\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0214 - val_loss: 0.0282\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0214 - val_loss: 0.0284\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0214 - val_loss: 0.0281\n",
      "Test Error: 0.262\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0214 - val_loss: 0.0287\n",
      "Test Error: 0.235\n",
      "(Real) Epoch: 78\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 187s 1ms/step - loss: 0.0215 - val_loss: 0.0275\n",
      "Test Error: 0.237\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0214 - val_loss: 0.0284\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0217 - val_loss: 0.0275\n",
      "Test Error: 0.252\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0215 - val_loss: 0.0279\n",
      "Test Error: 0.244\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0216 - val_loss: 0.0280\n",
      "Test Error: 0.245\n",
      "(Real) Epoch: 79\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 165s 1ms/step - loss: 0.0214 - val_loss: 0.0290\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0213 - val_loss: 0.0282\n",
      "Test Error: 0.241\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0214 - val_loss: 0.0286\n",
      "Test Error: 0.263\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0214 - val_loss: 0.0273\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0213 - val_loss: 0.0283\n",
      "Test Error: 0.242\n",
      "(Real) Epoch: 80\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0214 - val_loss: 0.0276\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0211 - val_loss: 0.0285\n",
      "Test Error: 0.22\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 177s 1ms/step - loss: 0.0212 - val_loss: 0.0283\n",
      "Test Error: 0.236\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0214 - val_loss: 0.0275\n",
      "Test Error: 0.239\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0211 - val_loss: 0.0291\n",
      "Test Error: 0.23\n",
      "(Real) Epoch: 81\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0214 - val_loss: 0.0282\n",
      "Test Error: 0.237\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0214 - val_loss: 0.0279\n",
      "Test Error: 0.244\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0211 - val_loss: 0.0280\n",
      "Test Error: 0.234\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 169s 1ms/step - loss: 0.0211 - val_loss: 0.0274\n",
      "Test Error: 0.229\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0212 - val_loss: 0.0277\n",
      "Test Error: 0.241\n",
      "(Real) Epoch: 82\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0212 - val_loss: 0.0270\n",
      "Test Error: 0.243\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 149s 1ms/step - loss: 0.0209 - val_loss: 0.0282\n",
      "Test Error: 0.229\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0213 - val_loss: 0.0274\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0210 - val_loss: 0.0274\n",
      "Test Error: 0.241\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0212 - val_loss: 0.0273\n",
      "Test Error: 0.226\n",
      "(Real) Epoch: 83\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0210 - val_loss: 0.0276\n",
      "Test Error: 0.251\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 174s 1ms/step - loss: 0.0211 - val_loss: 0.0278\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0208 - val_loss: 0.0275\n",
      "Test Error: 0.23\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0210 - val_loss: 0.0272\n",
      "Test Error: 0.246\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0212 - val_loss: 0.0283\n",
      "Test Error: 0.236\n",
      "(Real) Epoch: 84\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0212 - val_loss: 0.0281\n",
      "Test Error: 0.24\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0211 - val_loss: 0.0288\n",
      "Test Error: 0.231\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0210 - val_loss: 0.0273\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0210 - val_loss: 0.0273\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0208 - val_loss: 0.0282\n",
      "Test Error: 0.224\n",
      "(Real) Epoch: 85\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0208 - val_loss: 0.0278\n",
      "Test Error: 0.26\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 167s 1ms/step - loss: 0.0208 - val_loss: 0.0275\n",
      "Test Error: 0.219\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0210 - val_loss: 0.0275\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0210 - val_loss: 0.0279\n",
      "Test Error: 0.218\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0213 - val_loss: 0.0284\n",
      "Test Error: 0.211\n",
      "(Real) Epoch: 86\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0211 - val_loss: 0.0271\n",
      "Test Error: 0.224\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0207 - val_loss: 0.0277\n",
      "Test Error: 0.233\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0208 - val_loss: 0.0272\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0209 - val_loss: 0.0270\n",
      "Test Error: 0.236\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0209 - val_loss: 0.0272\n",
      "Test Error: 0.249\n",
      "(Real) Epoch: 87\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0211 - val_loss: 0.0276\n",
      "Test Error: 0.261\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0207 - val_loss: 0.0278\n",
      "Test Error: 0.227\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0208 - val_loss: 0.0281\n",
      "Test Error: 0.224\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0209 - val_loss: 0.0273\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0208 - val_loss: 0.0280\n",
      "Test Error: 0.217\n",
      "(Real) Epoch: 88\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0207 - val_loss: 0.0275\n",
      "Test Error: 0.237\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 150s 1ms/step - loss: 0.0209 - val_loss: 0.0286\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 152s 1ms/step - loss: 0.0207 - val_loss: 0.0274\n",
      "Test Error: 0.219\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0206 - val_loss: 0.0269\n",
      "Test Error: 0.215\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0206 - val_loss: 0.0278\n",
      "Test Error: 0.225\n",
      "(Real) Epoch: 89\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 176s 1ms/step - loss: 0.0208 - val_loss: 0.0282\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0207 - val_loss: 0.0283\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 178s 1ms/step - loss: 0.0207 - val_loss: 0.0271\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0207 - val_loss: 0.0266\n",
      "Test Error: 0.227\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0208 - val_loss: 0.0269\n",
      "Test Error: 0.239\n",
      "(Real) Epoch: 90\n",
      "Yielding phrases from #0 to #40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0206 - val_loss: 0.0279\n",
      "Test Error: 0.216\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0207 - val_loss: 0.0278\n",
      "Test Error: 0.242\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0206 - val_loss: 0.0273\n",
      "Test Error: 0.216\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 170s 1ms/step - loss: 0.0206 - val_loss: 0.0272\n",
      "Test Error: 0.208\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0206 - val_loss: 0.0278\n",
      "Test Error: 0.224\n",
      "(Real) Epoch: 91\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0207 - val_loss: 0.0279\n",
      "Test Error: 0.239\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 188s 1ms/step - loss: 0.0205 - val_loss: 0.0271\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 189s 1ms/step - loss: 0.0208 - val_loss: 0.0275\n",
      "Test Error: 0.218\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 182s 1ms/step - loss: 0.0206 - val_loss: 0.0270\n",
      "Test Error: 0.243\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0208 - val_loss: 0.0278\n",
      "Test Error: 0.244\n",
      "(Real) Epoch: 92\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0206 - val_loss: 0.0281\n",
      "Test Error: 0.228\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 168s 1ms/step - loss: 0.0207 - val_loss: 0.0284\n",
      "Test Error: 0.25\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0207 - val_loss: 0.0271\n",
      "Test Error: 0.219\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0205 - val_loss: 0.0264\n",
      "Test Error: 0.234\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 161s 1ms/step - loss: 0.0206 - val_loss: 0.0274\n",
      "Test Error: 0.224\n",
      "(Real) Epoch: 93\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0208 - val_loss: 0.0271\n",
      "Test Error: 0.231\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0206 - val_loss: 0.0277\n",
      "Test Error: 0.224\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0207 - val_loss: 0.0268\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0205 - val_loss: 0.0269\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0206 - val_loss: 0.0282\n",
      "Test Error: 0.217\n",
      "(Real) Epoch: 94\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 151s 1ms/step - loss: 0.0205 - val_loss: 0.0270\n",
      "Test Error: 0.213\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0202 - val_loss: 0.0281\n",
      "Test Error: 0.217\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 154s 1ms/step - loss: 0.0206 - val_loss: 0.0268\n",
      "Test Error: 0.221\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 153s 1ms/step - loss: 0.0204 - val_loss: 0.0271\n",
      "Test Error: 0.218\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 155s 1ms/step - loss: 0.0205 - val_loss: 0.0273\n",
      "Test Error: 0.233\n",
      "(Real) Epoch: 95\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0208 - val_loss: 0.0275\n",
      "Test Error: 0.227\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 186s 1ms/step - loss: 0.0205 - val_loss: 0.0282\n",
      "Test Error: 0.238\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 185s 1ms/step - loss: 0.0205 - val_loss: 0.0266\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 175s 1ms/step - loss: 0.0203 - val_loss: 0.0265\n",
      "Test Error: 0.239\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0205 - val_loss: 0.0262\n",
      "Test Error: 0.257\n",
      "(Real) Epoch: 96\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 173s 1ms/step - loss: 0.0206 - val_loss: 0.0272\n",
      "Test Error: 0.216\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 164s 1ms/step - loss: 0.0204 - val_loss: 0.0261\n",
      "Test Error: 0.248\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 160s 1ms/step - loss: 0.0205 - val_loss: 0.0278\n",
      "Test Error: 0.222\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 159s 1ms/step - loss: 0.0203 - val_loss: 0.0265\n",
      "Test Error: 0.229\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 162s 1ms/step - loss: 0.0204 - val_loss: 0.0266\n",
      "Test Error: 0.233\n",
      "(Real) Epoch: 97\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0203 - val_loss: 0.0272\n",
      "Test Error: 0.214\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0203 - val_loss: 0.0271\n",
      "Test Error: 0.233\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0205 - val_loss: 0.0279\n",
      "Test Error: 0.231\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0203 - val_loss: 0.0264\n",
      "Test Error: 0.232\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 180s 1ms/step - loss: 0.0204 - val_loss: 0.0276\n",
      "Test Error: 0.236\n",
      "(Real) Epoch: 98\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 181s 1ms/step - loss: 0.0203 - val_loss: 0.0273\n",
      "Test Error: 0.236\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 187s 1ms/step - loss: 0.0202 - val_loss: 0.0274\n",
      "Test Error: 0.213\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 187s 1ms/step - loss: 0.0201 - val_loss: 0.0278\n",
      "Test Error: 0.235\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 190s 1ms/step - loss: 0.0201 - val_loss: 0.0267\n",
      "Test Error: 0.218\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 172s 1ms/step - loss: 0.0202 - val_loss: 0.0269\n",
      "Test Error: 0.225\n",
      "(Real) Epoch: 99\n",
      "Yielding phrases from #0 to #40000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0204 - val_loss: 0.0270\n",
      "Test Error: 0.226\n",
      "Yielding phrases from #40000 to #80000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 171s 1ms/step - loss: 0.0204 - val_loss: 0.0276\n",
      "Test Error: 0.229\n",
      "Yielding phrases from #80000 to #120000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0203 - val_loss: 0.0269\n",
      "Test Error: 0.234\n",
      "Yielding phrases from #120000 to #160000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0202 - val_loss: 0.0262\n",
      "Test Error: 0.22\n",
      "Yielding phrases from #160000 to #200000\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/1\n",
      "144000/144000 [==============================] - 179s 1ms/step - loss: 0.0205 - val_loss: 0.0268\n",
      "Test Error: 0.234\n"
     ]
    }
   ],
   "source": [
    "loss, val, test = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"(Real) Epoch: %s\" % epoch)\n",
    "    # Initialize the generator\n",
    "    gen = training_generator()\n",
    "    \n",
    "    \n",
    "    l, v, t = [], [], []\n",
    "    for X, Y in gen:\n",
    "        h = model.fit(X, Y,\n",
    "                batch_size=batch_size,epochs=1, validation_split=0.1)\n",
    "        test_err = evaluate_misspelled(test_phrases[:1000], corrector)\n",
    "        print('Test Error:', test_err)\n",
    "        l.append(h.history['loss'][0])\n",
    "        v.append(h.history['val_loss'][0])\n",
    "        t.append(test_err)\n",
    "    \n",
    "    loss.append(hmean(l))\n",
    "    val.append(hmean(v))\n",
    "    test.append(hmean(t))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8241f87390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)\n",
    "plt.plot(val)\n",
    "plt.plot(test)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22892326419479278, 0.22503219045288073, 0.228476861809285]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_correct(input_phrases[:1000], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_correct(input_phrases[-1000:], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_misspelled(input_phrases[:1000], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.077"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_correct(test_phrases[:1000], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.214"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_misspelled(test_phrases[:1000], corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vect(input_texts, target_texts, model, training_vectorizer):\n",
    "    target_texts = wrap_with_delims(target_texts)\n",
    "    \n",
    "    #wrapped_target_texts = wrap_with_delims(target_texts)\n",
    "    X, Y = training_vectorizer(input_texts, target_texts)\n",
    "    loss = model.evaluate(X, Y)\n",
    "    print('\\nTesting loss: ', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 358us/step\n",
      "\n",
      "Testing loss:  0.04364998284727335\n"
     ]
    }
   ],
   "source": [
    "misspelled = [add_noise_to_string(p, .05) for p in test_phrases[:1000]]\n",
    "evaluate_vect(misspelled, test_phrases[:1000],\n",
    "              model, training_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fire',\n",
       " 'stop',\n",
       " 'come in',\n",
       " 'get out',\n",
       " \"i can't go\",\n",
       " \"i'm sorry\",\n",
       " 'he is busy',\n",
       " \"he's drunk\",\n",
       " \"i'll be late\",\n",
       " 'hold it beer',\n",
       " 'pus the button',\n",
       " 'coll me on my phone',\n",
       " 'hello boys and girls']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max encoder seq legth\n",
    "#max_encoder_seq_length = encoder_model.get_layer('encoder_inputs').input_shape[-1]\n",
    "phrases = ['fire', 'stp', 'comein', 'get ot', 'i cant go','im sorry', \n",
    "           'h is busi', 'hes drunk', 'ill be lat', 'hold mi beer', 'pus the buton', \n",
    "          'coll me on my phone', 'helo boys and girls']\n",
    "\n",
    "[corrector(phrase) for phrase in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    \"\"\"quick-n-dirty helper for saving models\"\"\"\n",
    "    print(\"Saving model\")\n",
    "    model.save('training.h5')\n",
    "    encoder_model.save('encoder.h5')\n",
    "    decoder_model.save('decoder.h5')\n",
    "\n",
    "    model_metadata = { 'input_token_index': input_token_index, \n",
    "                       'target_token_index': target_token_index,\n",
    "                       'max_encoder_seq_length': max_encoder_seq_length }\n",
    "\n",
    "    with open('model_metadata.pickle', 'wb') as f:\n",
    "        pickle.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
